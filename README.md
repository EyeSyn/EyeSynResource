# Research Artifacts of EyeSyn
This repository contains the research artifacts for paper ["EyeSyn: Psychology-inspired Eye Movement Synthesis for Gaze-based Activity Recognition"](), which is currently under submission to IPSN'22. 

If you have any questions on this repository or the related paper, please contact **ANONYMOUS AUTHOR** via *X [DOT] Y [AT] Z*.

## **Outline**

* [Demo Video](#1)

## 1. <span id="1"> Demo Video</span>

A short demo video of the gaze-based museum activity recognition is shown below. The demo is running on the Magic Leap One AR headset. The system leverages the gaze signals captured by the AR headset to continuouly track the interactive activity the user is performing, i.e., reading the text or viewing the painting. Then, based on the recognized user context, they AR system adjusts the digital content that is rendered in the user's view to enhance her engagment and learning experience.

**Note: by clicking the gif below, you will be redirected to the full demo video located on Youtube.**
[![Demo](https://github.com/EyeSyn/EyeSynResource/blob/main/demoGIF.gif)](https://youtu.be/s3GtVBg2JMg)




